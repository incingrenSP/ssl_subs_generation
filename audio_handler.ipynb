{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64286a5b-de84-4f01-9ded-722bef3c6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.requirements import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a7e97-6753-4b5f-b84d-00c411b4a034",
   "metadata": {},
   "source": [
    "## Audio Preprocessing\n",
    "- resample\n",
    "- stereo to mono\n",
    "- normalize\n",
    "- plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab02a39-b5f2-4187-9efe-057b6d2c4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryMappedAudioDataset(Dataset):\n",
    "    \"\"\"Memory-mapped audio dataset with robust error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_path, cache_dir='data/cache_mmap', top_db=20):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(metadata_path, sep=\"\\t\")\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.top_db = top_db\n",
    "        \n",
    "        dataset_name = Path(metadata_path).stem\n",
    "        meta_file = self.cache_dir / f\"{dataset_name}_meta.npz\"\n",
    "        audio_file = self.cache_dir / f\"{dataset_name}_audio.dat\"\n",
    "        \n",
    "        if not meta_file.exists():\n",
    "            print(\"Preprocessing audio (first time only)...\")\n",
    "            self._preprocess_all(audio_file, meta_file)\n",
    "        else:\n",
    "            print(f\"Loading metadata from cache...\")\n",
    "        \n",
    "        meta = np.load(meta_file, allow_pickle=True)\n",
    "        self.audio_shapes = meta['shapes']\n",
    "        self.audio_offsets = meta['offsets']\n",
    "        self.total_size = meta['total_size'].item()\n",
    "        \n",
    "        self.audio_mmap = np.memmap(audio_file, dtype='float32', mode='r', shape=(self.total_size,))\n",
    "        \n",
    "        print(f\"✓ Cache ready! {len(self.df)} samples\")\n",
    "        print(f\"  Total audio size: {self.total_size * 4 / (1024**3):.2f} GB\")\n",
    "    \n",
    "    def _preprocess_all(self, audio_file, meta_file):\n",
    "        \"\"\"Preprocess and save - single pass, in-order.\"\"\"\n",
    "        import time\n",
    "        start = time.time()\n",
    "        \n",
    "        print(\"Processing audio files...\")\n",
    "        \n",
    "        # Collect ALL audio first\n",
    "        all_audio_data = []\n",
    "        all_shapes = []\n",
    "        \n",
    "        for idx in tqdm(range(len(self.df)), desc=\"Loading & preprocessing\"):\n",
    "            path = self.df.iloc[idx]['path']\n",
    "            \n",
    "            # Load\n",
    "            waveform, sr = sf.read(path, always_2d=True)\n",
    "            waveform = np.array(waveform.T, dtype=np.float32)\n",
    "            \n",
    "            # Stereo to mono\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform.mean(axis=0)\n",
    "            else:\n",
    "                waveform = waveform[0]\n",
    "            \n",
    "            # Trim\n",
    "            trimmed, _ = librosa.effects.trim(waveform, top_db=self.top_db)\n",
    "            \n",
    "            # Normalize\n",
    "            max_val = np.abs(trimmed).max()\n",
    "            if max_val > 0:\n",
    "                trimmed = trimmed / max_val\n",
    "            \n",
    "            # Store\n",
    "            all_audio_data.append(trimmed.astype(np.float32))\n",
    "            all_shapes.append(len(trimmed))\n",
    "        \n",
    "        # Calculate total size\n",
    "        total_size = sum(all_shapes)\n",
    "        print(f\"Total samples: {total_size:,} ({total_size * 4 / (1024**3):.2f} GB)\")\n",
    "        \n",
    "        # Create offsets\n",
    "        offsets = np.zeros(len(all_shapes) + 1, dtype=np.int64)\n",
    "        np.cumsum(all_shapes, out=offsets[1:])\n",
    "        \n",
    "        # Write to file\n",
    "        print(\"Writing to memory-mapped file...\")\n",
    "        mmap = np.memmap(audio_file, dtype='float32', mode='w+', shape=(total_size,))\n",
    "        \n",
    "        for i in tqdm(range(len(all_audio_data)), desc=\"Writing\"):\n",
    "            start_pos = offsets[i]\n",
    "            end_pos = offsets[i + 1]\n",
    "            mmap[start_pos:end_pos] = all_audio_data[i]\n",
    "        \n",
    "        mmap.flush()\n",
    "        del mmap\n",
    "        \n",
    "        # Save metadata\n",
    "        np.savez(meta_file,\n",
    "                 shapes=np.array(all_shapes, dtype=np.int64),\n",
    "                 offsets=offsets,\n",
    "                 total_size=np.array(total_size, dtype=np.int64))\n",
    "        \n",
    "        print(f\"Done in {time.time() - start:.1f}s\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = self.audio_offsets[idx]\n",
    "        end = self.audio_offsets[idx + 1]\n",
    "        return torch.from_numpy(self.audio_mmap[start:end].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e2562-26d3-4fb5-ba0f-a4f3045d82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "ssl_train_dataset = MemoryMappedAudioDataset(\n",
    "    metadata_path='data/metadata_normal.tsv',\n",
    "    cache_dir='data/cache_mmap/ssl',\n",
    "    top_db=TOP_DB\n",
    ")\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1e708-c795-4b33-a9de-731cdf0b77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryMappedASRDataset(Dataset):\n",
    "    \"\"\"Memory-mapped ASR dataset with robust error handling.\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_path, tokenizer, cache_dir='data/cache_mmap_asr', top_db=20):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(metadata_path, sep=\"\\t\")\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.top_db = top_db\n",
    "        \n",
    "        dataset_name = Path(metadata_path).stem\n",
    "        meta_file = self.cache_dir / f\"{dataset_name}_meta.npz\"\n",
    "        audio_file = self.cache_dir / f\"{dataset_name}_audio.dat\"\n",
    "        \n",
    "        if not meta_file.exists():\n",
    "            print(\"Preprocessing audio (first time only)...\")\n",
    "            self._preprocess_all(audio_file, meta_file)\n",
    "        else:\n",
    "            print(f\"Loading metadata from cache...\")\n",
    "        \n",
    "        meta = np.load(meta_file, allow_pickle=True)\n",
    "        self.audio_shapes = meta['shapes']\n",
    "        self.audio_offsets = meta['offsets']\n",
    "        self.total_size = meta['total_size'].item()\n",
    "        \n",
    "        self.audio_mmap = np.memmap(audio_file, dtype='float32', mode='r', shape=(self.total_size,))\n",
    "        \n",
    "        print(f\"✓ Cache ready! {len(self.df)} samples\")\n",
    "        print(f\"  Total audio size: {self.total_size * 4 / (1024**3):.2f} GB\")\n",
    "        \n",
    "        # Encode transcripts\n",
    "        print(\"Encoding transcripts...\")\n",
    "        self.encode_transcripts = []\n",
    "        for t in tqdm(self.df['transcript'].tolist(), desc=\"Encoding\"):\n",
    "            encoded = tokenizer.encode(t)\n",
    "            encoded = [i if i >= 0 else 0 for i in encoded]\n",
    "            self.encode_transcripts.append(torch.tensor(encoded, dtype=torch.long))\n",
    "    \n",
    "    def _preprocess_all(self, audio_file, meta_file):\n",
    "        \"\"\"Preprocess and save - single pass, in-order.\"\"\"\n",
    "        import time\n",
    "        start = time.time()\n",
    "        \n",
    "        print(\"Processing audio files...\")\n",
    "        \n",
    "        # Collect ALL audio first\n",
    "        all_audio_data = []\n",
    "        all_shapes = []\n",
    "        \n",
    "        for idx in tqdm(range(len(self.df)), desc=\"Loading & preprocessing\"):\n",
    "            path = self.df.iloc[idx]['path']\n",
    "            \n",
    "            # Load\n",
    "            waveform, sr = sf.read(path, always_2d=True)\n",
    "            waveform = np.array(waveform.T, dtype=np.float32)\n",
    "            \n",
    "            # Stereo to mono\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform.mean(axis=0)\n",
    "            else:\n",
    "                waveform = waveform[0]\n",
    "            \n",
    "            # Trim\n",
    "            trimmed, _ = librosa.effects.trim(waveform, top_db=self.top_db)\n",
    "            \n",
    "            # Normalize\n",
    "            max_val = np.abs(trimmed).max()\n",
    "            if max_val > 1:\n",
    "                trimmed = trimmed / max_val\n",
    "            \n",
    "            # Store\n",
    "            all_audio_data.append(trimmed.astype(np.float32))\n",
    "            all_shapes.append(len(trimmed))\n",
    "        \n",
    "        # Calculate total size\n",
    "        total_size = sum(all_shapes)\n",
    "        print(f\"Total samples: {total_size:,} ({total_size * 4 / (1024**3):.2f} GB)\")\n",
    "        \n",
    "        # Create offsets\n",
    "        offsets = np.zeros(len(all_shapes) + 1, dtype=np.int64)\n",
    "        np.cumsum(all_shapes, out=offsets[1:])\n",
    "        \n",
    "        # Write to file\n",
    "        print(\"Writing to memory-mapped file...\")\n",
    "        mmap = np.memmap(audio_file, dtype='float32', mode='w+', shape=(total_size,))\n",
    "        \n",
    "        for i in tqdm(range(len(all_audio_data)), desc=\"Writing\"):\n",
    "            start_pos = offsets[i]\n",
    "            end_pos = offsets[i + 1]\n",
    "            mmap[start_pos:end_pos] = all_audio_data[i]\n",
    "        \n",
    "        mmap.flush()\n",
    "        del mmap\n",
    "        \n",
    "        # Save metadata\n",
    "        np.savez(meta_file,\n",
    "                 shapes=np.array(all_shapes, dtype=np.int64),\n",
    "                 offsets=offsets,\n",
    "                 total_size=np.array(total_size, dtype=np.int64))\n",
    "        \n",
    "        print(f\"✓ Done in {time.time() - start:.1f}s\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = self.audio_offsets[idx]\n",
    "        end = self.audio_offsets[idx + 1]\n",
    "        waveform = torch.from_numpy(self.audio_mmap[start:end].copy())\n",
    "        target = self.encode_transcripts[idx]\n",
    "        return waveform, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97357f32-2dba-4201-b134-766eb504564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizer import *\n",
    "tokenizer = Tokenizer.load(os.path.join(\"data\", \"tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2488b5-f2be-4892-b551-1343ef1696f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Subsequent runs: loads from cache (FAST!)\n",
    "asr_train_dataset = MemoryMappedASRDataset(\n",
    "    metadata_path='data/metadata_normal.tsv',\n",
    "    tokenizer=tokenizer,\n",
    "    cache_dir='data/cache_mmap/asr',\n",
    "    top_db=TOP_DB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6933cc0-a5f1-4922-a786-267e727965dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
