{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64286a5b-de84-4f01-9ded-722bef3c6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.requirements import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a7e97-6753-4b5f-b84d-00c411b4a034",
   "metadata": {},
   "source": [
    "## Audio Preprocessing\n",
    "- resample\n",
    "- stereo to mono\n",
    "- normalize\n",
    "- plot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f32ee27c-4762-47d3-a387-81f667a2a4de",
   "metadata": {},
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_path, device='cpu', target_sr=16000):\n",
    "        self.audio_path = audio_path\n",
    "        self.device = device\n",
    "        self.target_sr = target_sr\n",
    "        self.data = []\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        for dir1 in os.listdir(self.audio_path):\n",
    "            if os.path.splitext(dir1)[1] == '.tsv':\n",
    "                continue\n",
    "            for dir2 in tqdm(os.listdir(os.path.join(self.audio_path, dir1))):\n",
    "                if os.path.splitext(dir2)[1] == '.tsv':\n",
    "                    continue\n",
    "                waveform, sr = torchaudio.load(os.path.join(self.audio_path, dir1, dir2))\n",
    "                waveform, sr = self.audio_preprocess(waveform, sr, self.target_sr)\n",
    "                self.data.append(waveform.squeeze(0))\n",
    "\n",
    "    def audio_preprocess(self, waveform, sr, target_sr):\n",
    "        # resample\n",
    "        resampler = T.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "        # stereo -> mono\n",
    "        waveform_mono = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        waveform = waveform_mono\n",
    "    \n",
    "        # normalize\n",
    "        waveform = waveform / waveform.abs().max()\n",
    "    \n",
    "        return waveform, target_sr"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a12dade-5fe8-4a8f-bd7c-8b68e6c23fa5",
   "metadata": {},
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_path, target_sr=16000, transform=None):\n",
    "        self.audio_path = audio_path\n",
    "        self.target_sr = target_sr\n",
    "        self.transform = transform\n",
    "        self.file_list = []\n",
    "        self._gather_files()\n",
    "\n",
    "    def _gather_files(self):\n",
    "        for dir1 in os.listdir(self.audio_path):\n",
    "            if dir1.endswith('.tsv'):\n",
    "                continue\n",
    "            subdir = os.path.join(self.audio_path, dir1)\n",
    "            for dir2 in tqdm(os.listdir(subdir)):\n",
    "                if dir2.endswith('.tsv'):\n",
    "                    continue\n",
    "                path = os.path.join(subdir, dir2)\n",
    "                self.file_list.append(path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_list[idx]\n",
    "        waveform, sr = sf.read(path, always_2d=True)\n",
    "        waveform = torch.Tensor(waveform.T)\n",
    "\n",
    "        if sr != self.target_sr:\n",
    "            resampler = T.Resampler(orig_freq=sr, new_freq=self.target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        waveform = waveform / (waveform.abs().max() + 1e-8)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        return waveform.squeeze(0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6def5b65-3177-46c9-91b9-eba46cd71f95",
   "metadata": {},
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, metadata_path):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(metadata_path, sep=\"\\t\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['path']\n",
    "        waveform, sr = sf.read(path, always_2d=True)\n",
    "        waveform = torch.tensor(waveform.T)\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        waveform = waveform / torch.max(torch.abs(waveform))\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "\n",
    "        return waveform.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac6df1-50b2-4419-a8b9-4d2cc956d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.requirements import *\n",
    "\n",
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, metadata_path, tokenizer):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(metadata_path, sep=\"\\t\")\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        waveform, sr = sf.read(row['path'], always_2d=True)\n",
    "            \n",
    "        waveform = torch.tensor(waveform.T, dtype=torch.float32)\n",
    "\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        waveform = waveform / torch.max(torch.abs(waveform))\n",
    "            \n",
    "        target = self.tokenizer.encode(row['transcript'])\n",
    "        \n",
    "        return waveform.squeeze(0), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f071a3-cc2f-456c-b9b1-d9c25e36f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, corpus_path, add_blank=True):        \n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            lines = [unicodedata.normalize('NFC', l.strip()) for l in f]\n",
    "\n",
    "        tokens = []\n",
    "        for line in lines:\n",
    "            tokens.extend(self.tokenize(line))\n",
    "\n",
    "        counter = Counter(tokens)\n",
    "        self.vocab = sorted(counter.keys())\n",
    "\n",
    "        if add_blank:\n",
    "            self.vocab = ['<blank>'] + self.vocab\n",
    "\n",
    "        self.token_to_id = {t: i for i, t in enumerate(self.vocab)}\n",
    "        self.id_to_token = {i: t for t, i in self.token_to_id.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "        return regex.findall(r'\\X', text)\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.token_to_id[t] for t in tokens if t in self.token_to_id]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.id_to_token[i] for i in ids if i in self.id_to_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7414d3b-b0c4-4c4c-af2f-90d54a45c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_padding_asr(batch):\n",
    "    waveforms, targets = zip(*batch)\n",
    "    waveforms = rnn_utils.pad_sequence(waveforms, batch_first=True, padding_value=0)\n",
    "    targets = rnn_utils.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    \n",
    "    waveforms = waveforms.unsqueeze(1)\n",
    "    \n",
    "    input_len = torch.tensor([wave.shape[-1] for wave in waveforms], dtype=torch.long)\n",
    "    target_len = torch.tensor([len(target) for target in targets], dtype=torch.long)\n",
    "    \n",
    "    return waveforms, targets, input_len, target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c4c54-8838-42eb-aa93-f4d411ca7e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zipping two lists\n",
    "names = [\"Alice\", \"Octavia\", \"Nicole\"]\n",
    "scores = [85, 90, 88]\n",
    "\n",
    "zipped_data = zip(names, scores)\n",
    "print(list(zipped_data))\n",
    "\n",
    "# Creating a dictionary\n",
    "keys = [\"name\", \"age\", \"city\"]\n",
    "values = [\"Rerir\", 525, \"Khaenri'ah\"]\n",
    "\n",
    "person_dict = dict(zip(keys, values))\n",
    "print(person_dict)\n",
    "\n",
    "# Unzipping\n",
    "zipped_pairs = [('Nefer', 1), ('Aino', 2), ('Lauma', 3), ('Kuki', 4)]\n",
    "letters, numbers = zip(*zipped_pairs)\n",
    "print(f\"Letters: {letters}\")\n",
    "print(f\"Numbers: {numbers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2760f-2b18-4e93-ba73-1f5682e4c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_text(corpus_path):\n",
    "    all_text = \"\"\n",
    "    for file in tqdm(glob.glob(corpus_path + \"/**/*.txt\", recursive=True)):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_text += f.read() + \"\\n\"\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31941cbc-9ec9-4542-b535-a84a88a81dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = os.path.join(\"data\", \"corpus.txt\")\n",
    "if not os.path.exists(text_path):\n",
    "    path = os.path.join(\"data\", \"text\")\n",
    "    filename = \"corpus.txt\"\n",
    "    text = load_corpus_text(path)\n",
    "    with open(os.path.join(\"data\", filename), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "tokenizer = Tokenizer(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc1f7f-d23d-4aa2-958f-037314efde93",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_data = ASRDataset(os.path.join(\"data\", \"metadata.tsv\"), tokenizer)\n",
    "asr_dl = DataLoader(\n",
    "    dataset = asr_data,\n",
    "    batch_size = 8,\n",
    "    pin_memory = True,\n",
    "    collate_fn = collate_padding_asr,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0b3cc-7560-489a-8d82-dfe36f31f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in asr_dl:\n",
    "    waveform, target, _, _ = batch\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d964e17d-389a-4f08-bc2a-7e0e81c779e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.requirements import *\n",
    "import IPython\n",
    "\n",
    "idx = 3\n",
    "df = pd.read_csv(os.path.join(\"data\", \"metadata.tsv\"), sep=\"\\t\")\n",
    "row = df.iloc[idx]\n",
    "path = row['path']\n",
    "transcript = row['transcript']\n",
    "waveform, sr = sf.read(path, always_2d=True)\n",
    "waveform = torch.tensor(waveform.T, dtype=torch.float32)\n",
    "print(waveform.shape)\n",
    "print(transcript)\n",
    "IPython.display.Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b426c9-5eca-4017-88b8-69ddcff089cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(\"घर जग्गा कारोबारमा आत्मविश्वास गुमेको वर्ष बैंकिङ प्रणाली सेयर बजार\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6dfbb3-9d07-46fe-aab6-b48e11f0c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"घर जग्गा कारोबारमा आत्मविश्वास गुमेको वर्ष बैंकिङ प्रणाली सेयर बजार\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa349f0-035b-46b8-ac45-cc53961eb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.requirements import *\n",
    "from src.audio_handler import AudioDataset, collate_padding\n",
    "\n",
    "ssl_data = AudioDataset(os.path.join(\"data\", \"metadata.tsv\"))\n",
    "ssl_dl = DataLoader(\n",
    "    dataset = ssl_data,\n",
    "    batch_size = 8,\n",
    "    pin_memory = True,\n",
    "    collate_fn = collate_padding,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e0874-0b2f-42a2-be53-c36562374d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"data\", \"corpus.txt\")\n",
    "if not os.path.exists(path):\n",
    "    # text = \"I am the one, I'm here, I've infiltrated.\\nYour time has come to be downgraded.\\nI've already won as far as I can see,\\nso keep your eyes on me.\"\n",
    "    text = \"घर जग्गा कारोबारमा आत्मविश्वास गुमेको वर्ष बैंकिङ प्रणाली सेयर बजार र घरजग्गाले गएको वर्ष कम्तीमा एउटा साझा समस्या भोगे विश्वासको\\nमेलमिलापको केन्द्र राष्ट्रियतानेपाली कांग्रेसले पर्वका रूपमा मनाउने गरेको राष्ट्रिय एकता तथा मेलमिलाप दिवस हिजो पनि देशैभरि मनाइयो\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(\"text written\")\n",
    "else:\n",
    "    print(\"file exists bruh\")\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [unicodedata.normalize('NFC', l.strip()) for l in f]\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582fe52-c0e8-407e-b071-a85d3e1aa21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "        return regex.findall(r'\\X', text)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for line in lines:\n",
    "    tokens.extend(tokenize(line))\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ade801-d2bb-43f8-a62b-1f1b29f1b5eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = Counter(tokens)\n",
    "vocab = sorted(counter.keys())\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088a7d7-9f96-49f6-85d2-8990f3668522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_w_blank = ['<blank>'] + vocab\n",
    "vocab_w_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7274f3ee-34e1-4ec1-a43e-9cc83edb0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {t: i for i, t in enumerate(vocab)}\n",
    "token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bcd5e-3ed3-49bd-868d-1185b2035255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"data\", \"metadata.tsv\"), sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a70ba4-ae9e-4284-b178-b169e281444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = df[\"transcript\"].tolist()\n",
    "all_chars = set(\"\".join(transcripts))\n",
    "unique_vocabs = list(all_chars)\n",
    "vocab_size = len(unique_vocabs)\n",
    "unique_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab02a39-b5f2-4187-9efe-057b6d2c4742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
