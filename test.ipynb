{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2715e-c09e-429e-b6fc-4b0ebd022cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.requirements import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0bd19-469c-4de3-a8af-0053f700b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.requirements import *\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, hidden_dim, kernel_size=3, stride=5, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "        \n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=4, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "        \n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=4, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "        \n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=4, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class ContextModule(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, z):\n",
    "        output, _ = self.gru(z)\n",
    "        return output\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, hidden_dim, proj_dim):\n",
    "        super().__init__()\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(proj_dim, proj_dim),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.project(x)\n",
    "\n",
    "class SSLModel(nn.Module):\n",
    "    def __init__(self, feat_dim=128, proj_dim=128, m=0.999):\n",
    "        super().__init__()\n",
    "\n",
    "        # online networks\n",
    "        self.encoder = FeatureEncoder()\n",
    "        self.context = ContextModule(feat_dim, feat_dim)        \n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(feat_dim, proj_dim),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "        self.predictor = Predictor(proj_dim, proj_dim)\n",
    "\n",
    "        # momentum networks (exponential moving average)\n",
    "        self.momentum = m\n",
    "        self.encoder_m = copy.deepcopy(self.encoder)\n",
    "        self.context_m = copy.deepcopy(self.context)\n",
    "        self.projector_m = copy.deepcopy(self.projector)\n",
    "\n",
    "        self._init_momentum_encoder()\n",
    "\n",
    "    def _init_momentum_encoder(self):\n",
    "        for p in self.encoder_m.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.context_m.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.projector_m.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_momentum(self):\n",
    "        m = self.momentum\n",
    "        for online, target in [(self.encoder, self.encoder_m),\n",
    "                               (self.context, self.context_m),\n",
    "                               (self.projector, self.projector_m)]:\n",
    "            for p_o, p_t in zip(online.parameters(), target.parameters()):\n",
    "                p_t.data = p_t.data * m + p_o.data * (1.0 - m)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.transpose(1, 2)\n",
    "        return self.context(z)\n",
    "\n",
    "    def forward(self, x, mask_prob=0.065, mask_length=10):\n",
    "        B = x.size(0)\n",
    "    \n",
    "        # online branch\n",
    "        z1 = self.encoder(x).transpose(1, 2)\n",
    "        T = z1.size(1)\n",
    "    \n",
    "        mask1 = compute_mask_indices(\n",
    "            B, T,\n",
    "            mask_prob=mask_prob,\n",
    "            mask_length=mask_length,\n",
    "            device=z1.device\n",
    "        )\n",
    "    \n",
    "        z1 = apply_mask(z1, mask1)\n",
    "        c1 = self.context(z1)\n",
    "        c1_seq = self.context(z1)\n",
    "        c1 = masked_mean(c1, mask1)\n",
    "    \n",
    "        h1 = self.projector(c1)\n",
    "        p1 = self.predictor(h1)\n",
    "    \n",
    "        # momentum branch\n",
    "        with torch.no_grad():\n",
    "            z2 = self.encoder_m(x).transpose(1, 2)\n",
    "    \n",
    "            mask2 = compute_mask_indices(\n",
    "                B, T,\n",
    "                mask_prob=mask_prob,\n",
    "                mask_length=mask_length,\n",
    "                device=z2.device\n",
    "            )\n",
    "    \n",
    "            z2 = apply_mask(z2, mask2)\n",
    "            c2 = self.context_m(z2)\n",
    "            c2 = masked_mean(c2, mask2)\n",
    "    \n",
    "            h2 = self.projector_m(c2)\n",
    "    \n",
    "        # calc loss\n",
    "        loss = byol_loss(p1, h2) + 0.05 * variance_loss(c1_seq)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def compute_mask_indices(B, T, mask_prob=0.05, mask_length=10, device=\"cpu\"):\n",
    "    mask = torch.zeros((B, T), dtype=torch.bool, device=device)\n",
    "    num_masked_steps = int(mask_prob * T)\n",
    "    num_spans = max(1, num_masked_steps // mask_length)\n",
    "\n",
    "    for b in range(B):\n",
    "        possible_starts = torch.arange(T - mask_length, device=device)\n",
    "        perm = torch.randperm(len(possible_starts), device=device)\n",
    "        span_starts = possible_starts[perm[:num_spans]]\n",
    "\n",
    "        for s in span_starts:\n",
    "            mask[b, s: s + mask_length] = True\n",
    "\n",
    "    return mask\n",
    "\n",
    "def variance_loss(z, eps=1e-4):\n",
    "    z = z.reshape(-1, z.size(-1))\n",
    "    std = torch.sqrt(z.var(dim=0, unbiased=False) + eps)\n",
    "    return torch.mean(F.relu(1.0 - std))\n",
    "\n",
    "\n",
    "def apply_mask(z, mask):\n",
    "    z = z.clone()\n",
    "    z[mask] = 0.0\n",
    "    return z\n",
    "\n",
    "def masked_mean(c, mask):\n",
    "    valid = (~mask).unsqueeze(-1).float()\n",
    "    return (c * valid).sum(dim=1) / valid.sum(dim=1).clamp(min=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed070d27-07d9-4fc9-a70c-b54f84e69dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLModel(nn.Module):\n",
    "    def __init__(self, feat_dim=128, proj_dim=128, m=0.999):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = FeatureEncoder(hidden_dim=feat_dim)\n",
    "        self.context = ContextModule(feat_dim, feat_dim)\n",
    "        \n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(feat_dim, proj_dim),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "        self.predictor = Predictor(proj_dim, proj_dim)\n",
    "\n",
    "        self.momentum = m\n",
    "        self.encoder_m = copy.deepcopy(self.encoder)\n",
    "        self.projector_m = copy.deepcopy(self.projector)\n",
    "        \n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, feat_dim))\n",
    "        nn.init.normal_(self.mask_token, std=0.02)\n",
    "\n",
    "        self._init_momentum_encoder()\n",
    "\n",
    "    def _init_momentum_encoder(self):\n",
    "        for p in list(self.encoder_m.parameters()) + list(self.projector_m.parameters()):\n",
    "            p.requires_grad = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_momentum(self):\n",
    "        m = self.momentum\n",
    "        for online, target in [(self.encoder, self.encoder_m),\n",
    "                               (self.projector, self.projector_m)]:\n",
    "            for p_o, p_t in zip(online.parameters(), target.parameters()):\n",
    "                p_t.data = p_t.data * m + p_o.data * (1.0 - m)\n",
    "\n",
    "    def forward(self, x, mask_prob=0.065, mask_length=10):\n",
    "        B, C, L = x.size()\n",
    "        \n",
    "        z = self.encoder(x).transpose(1, 2)  # [B, T, feat_dim]\n",
    "        T = z.size(1)\n",
    "\n",
    "        mask = compute_mask_indices(B, T, mask_prob, mask_length, z.device)\n",
    "        \n",
    "        z_masked = z.clone()\n",
    "        z_masked[mask] = self.mask_token.to(z.dtype)\n",
    "        \n",
    "        c = self.context(z_masked)\n",
    "        p = self.predictor(self.projector(c)) # [B, T, proj_dim]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z_m = self.encoder_m(x).transpose(1, 2)\n",
    "            target_h = self.projector_m(z_m) # [B, T, proj_dim]\n",
    "\n",
    "        loss = self.byol_masked_loss(p, target_h, mask)\n",
    "        \n",
    "        loss += 0.05 * variance_loss(z)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def byol_masked_loss(self, p, z, mask):\n",
    "        p_masked = p[mask] \n",
    "        z_masked = z[mask]\n",
    "        \n",
    "        p_norm = F.normalize(p_masked, dim=-1)\n",
    "        z_norm = F.normalize(z_masked, dim=-1)\n",
    "        \n",
    "        return (2 - 2 * (p_norm * z_norm).sum(dim=-1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09207818-26d1-4262-9137-2dce3e3cf0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input! Please provide an integer.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from random import random\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "def rain_drop(length_of_field=1):\n",
    "    \"\"\"\n",
    "    Simulate a random rain drop within the square field.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (0.5 - random()) * length_of_field,\n",
    "        (0.5 - random()) * length_of_field\n",
    "    ]\n",
    "\n",
    "\n",
    "def is_point_in_circle(point, length_of_field=1):\n",
    "    \"\"\"\n",
    "    Check if a point lies within the inscribed circle.\n",
    "    \"\"\"\n",
    "    return point[0] ** 2 + point[1] ** 2 <= (length_of_field / 2) ** 2\n",
    "\n",
    "\n",
    "def plot_rain_drops(drops_in_circle, drops_out_of_circle, length_of_field=1, format=\"png\"):\n",
    "    \"\"\"\n",
    "    Plot the raindrops inside and outside the circle.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlim(-length_of_field / 2, length_of_field / 2)\n",
    "    plt.ylim(-length_of_field / 2, length_of_field / 2)\n",
    "\n",
    "    plt.scatter(\n",
    "        [e[0] for e in drops_in_circle],\n",
    "        [e[1] for e in drops_in_circle],\n",
    "        color=\"blue\",\n",
    "        label=\"Drops in circle\"\n",
    "    )\n",
    "\n",
    "    plt.scatter(\n",
    "        [e[0] for e in drops_out_of_circle],\n",
    "        [e[1] for e in drops_out_of_circle],\n",
    "        color=\"black\",\n",
    "        label=\"Drops out of circle\"\n",
    "    )\n",
    "\n",
    "    plt.legend(loc=\"center\")\n",
    "    total = len(drops_in_circle) + len(drops_out_of_circle)\n",
    "    pi_est = 4 * len(drops_in_circle) / total\n",
    "\n",
    "    plt.title(\n",
    "        f\"{total} drops: {len(drops_in_circle)} in circle, \"\n",
    "        f\"π ≈ {pi_est:.4f}\"\n",
    "    )\n",
    "    plt.savefig(f\"rain_drops.{format}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_pi_estimates(pi_estimate, total_drops):\n",
    "    \"\"\"\n",
    "    Plot π estimates versus number of drops.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.scatter(range(1, total_drops + 1), pi_estimate, s=1)\n",
    "    plt.hlines(pi, 0, total_drops, color=\"black\", linestyle=\"dashed\", label=\"Actual π\")\n",
    "    plt.xlim(0, total_drops)\n",
    "\n",
    "    plt.title(\"π estimate vs number of rain drops\")\n",
    "    plt.xlabel(\"Number of rain drops\")\n",
    "    plt.ylabel(\"π\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"pi_estimate_{total_drops}_drops.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def rain(number_of_drops, length_of_field=1, plot=True, dynamic=False):\n",
    "    \"\"\"\n",
    "    Simulate raindrops to estimate π.\n",
    "    \"\"\"\n",
    "    number_of_drops_in_circle = 0\n",
    "    drops_in_circle = []\n",
    "    drops_out_of_circle = []\n",
    "    pi_estimate = []\n",
    "\n",
    "    for k in range(number_of_drops):\n",
    "        drop = rain_drop(length_of_field)\n",
    "\n",
    "        if is_point_in_circle(drop, length_of_field):\n",
    "            drops_in_circle.append(drop)\n",
    "            number_of_drops_in_circle += 1\n",
    "        else:\n",
    "            drops_out_of_circle.append(drop)\n",
    "\n",
    "        pi_estimate.append(4 * number_of_drops_in_circle / (k + 1))\n",
    "\n",
    "        if dynamic:\n",
    "            plot_rain_drops(drops_in_circle, drops_out_of_circle, length_of_field)\n",
    "\n",
    "    if plot and not dynamic:\n",
    "        plot_rain_drops(drops_in_circle, drops_out_of_circle, length_of_field)\n",
    "        plot_pi_estimates(pi_estimate, number_of_drops)\n",
    "\n",
    "    return number_of_drops_in_circle, number_of_drops\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        if len(sys.argv) > 1:\n",
    "            number_of_drops = int(sys.argv[1])\n",
    "        else:\n",
    "            number_of_drops = int(input(\"Enter the number of drops: \"))\n",
    "\n",
    "        result = rain(number_of_drops, plot=True, dynamic=False)\n",
    "\n",
    "        print(\"----------------------\")\n",
    "        print(f\"{number_of_drops} drops\")\n",
    "        print(f\"π estimated as: {4 * result[0] / result[1]:.6f}\")\n",
    "        print(\"----------------------\")\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Invalid input! Please provide an integer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202522f-225b-4a70-a1b2-18d642fef433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
