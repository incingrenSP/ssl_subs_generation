{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022dfda-38b4-441f-ac2b-d910f4b559f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.requirements import *\n",
    "from src.tokenizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f63a7-215f-4fcc-8158-77f727680c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMATTING_REMOVE = {'\\u200d', '\\u200c', '\\u200b', '\\ufeff'}\n",
    "JUNK = {'\\u200e', '\\u200f', '\\xa0', '“', '”'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33887a04-f905-4a6f-92b0-3c6edda9c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_char(ch):\n",
    "    code = ord(ch)\n",
    "\n",
    "    # 1. Devanagari block (U+0900 to U+097F)\n",
    "    if 0x0900 <= code <= 0x097F:\n",
    "        return True\n",
    "\n",
    "    # 2. Standard Latin Digits (0-9) \n",
    "    if '0' <= ch <= '9':\n",
    "        return False\n",
    "\n",
    "    # 3. Basic Punctuation & Whitespace\n",
    "    if ch in \" \\n\\t.,?!-()\\\"'।॥\":\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4d76a-a908-447a-9257-3209f5dd58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transcript(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    \n",
    "    cleaned = []\n",
    "    for ch in text:\n",
    "        if ch in FORMATTING_REMOVE:\n",
    "            continue\n",
    "        \n",
    "        cleaned.append(ch)\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    for junk in JUNK:\n",
    "        text = text.replace(junk, '')\n",
    "    \n",
    "    text = text.replace('ऱ', 'र')\n",
    "    \n",
    "    return \"\".join([ch for ch in text if is_valid_char(ch)])\n",
    "\n",
    "input_path = os.path.join(\"data\", \"metadata.tsv\")\n",
    "\n",
    "df = pd.read_csv(input_path, sep=\"\\t\")\n",
    "\n",
    "print(\"Cleaning transcripts...\")\n",
    "df['transcript'] = df['transcript'].apply(clean_transcript)\n",
    "\n",
    "output_path = os.path.join(\"data\", \"metadata_normal.tsv\")\n",
    "df.to_csv(output_path, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Done! Cleaned file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806793f-722b-4957-a7e3-cc943453ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = os.path.join(\"data\", \"tokenizer.json\")\n",
    "tokenizer = Tokenizer.load(token_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4492a05-2620-4dfc-a715-fd67cb6ea889",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = set(\"\".join(df['transcript'].astype(str)))\n",
    "missing = [c for c in all_chars if c not in tokenizer.token_to_id]\n",
    "\n",
    "if missing:\n",
    "    print(f\"Still missing these characters: {repr(''.join(missing))}\")\n",
    "else:\n",
    "    print(\"Tokenizer is fully compatible with the cleaned metadata!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40e51c-a580-4a40-a585-8fce7b6c0c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
