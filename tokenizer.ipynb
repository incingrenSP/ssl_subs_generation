{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588747a7-c32e-4a4f-bde5-402a3d11f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.requirements import *\n",
    "from src.audio_handler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa89f42-d5b6-4903-a12e-d3b7beceb1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab=None):\n",
    "        self.blank_token = \"<blank>\"\n",
    "        self.blank_id = 0\n",
    "        \n",
    "        if vocab is not None:\n",
    "            self.token_to_id = vocab[\"tokens\"]\n",
    "            self.id_to_token = {int(k): v for k, v in vocab[\"ids\"].items()}\n",
    "            self.vocab_size = vocab[\"size\"]\n",
    "\n",
    "            return\n",
    "            \n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "    # Normalization\n",
    "    def normalize(self, text: str) -> str:\n",
    "        return unicodedata.normalize(\"NFD\", text)\n",
    "\n",
    "    def denormalize(self, text: str) -> str:\n",
    "        return unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "    # Vocab\n",
    "    def build_vocab(self, texts):\n",
    "        counter = Counter()\n",
    "\n",
    "        for text in texts:\n",
    "            text = self.normalize(text)\n",
    "            for ch in text:\n",
    "                counter[ch] += 1\n",
    "\n",
    "        # <blank> : id[0]\n",
    "        # \" \" : id[1]\n",
    "        self.token_to_id = {self.blank_token: self.blank_id}\n",
    "        self.id_to_token = {self.blank_id: self.blank_token}\n",
    "\n",
    "        next_id = 1\n",
    "        for ch, _ in counter.most_common():\n",
    "            self.token_to_id[ch] = next_id\n",
    "            self.id_to_token[next_id] = ch\n",
    "            next_id += 1\n",
    "\n",
    "        self.vocab_size = next_id\n",
    "\n",
    "    # Encoding / Decoding\n",
    "    def encode(self, text: str):\n",
    "        text = self.normalize(text)\n",
    "        ids = []\n",
    "\n",
    "        for ch in text:\n",
    "            if ch not in self.token_to_id:\n",
    "                raise ValueError(f\"Unknown character: {repr(ch)}\")\n",
    "            ids.append(self.token_to_id[ch])\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        chars = []\n",
    "\n",
    "        for i in ids:\n",
    "            if i == self.blank_id:\n",
    "                continue\n",
    "            chars.append(self.id_to_token[i])\n",
    "\n",
    "        text = \"\".join(chars)\n",
    "        return self.denormalize(text)\n",
    "\n",
    "    def save(self, path):\n",
    "        data = {\"tokens\" : self.token_to_id, \"ids\" : self.id_to_token, \"size\" : self.vocab_size}\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return Tokenizer(vocab=data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4ba6b-70a5-430e-95de-be7704d05810",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"data\", \"text\")\n",
    "text = load_text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34ed04-d387-491a-933e-70e110e2847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = os.path.join(\"data\", \"tokenizer.json\")\n",
    "if not os.path.exists(token_path):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.build_vocab(text)\n",
    "    tokenizer.save(token_path)\n",
    "else:\n",
    "    tokenizer = Tokenizer.load(token_path)\n",
    "    \n",
    "print(\"Vocab size:\", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435851b-7411-4510-944a-690c45058c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.build_vocab(text)\n",
    "\n",
    "# print(\"Vocab size:\", len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214b04c-0241-4d7f-a7c3-94a3cfbdaf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e4cef-42ce-49d9-9c65-defc50e1fc27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
