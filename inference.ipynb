{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa147c7-96c1-4b6c-b668-4b644263b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.requirements import *\n",
    "from src.ssl_model import *\n",
    "from src.asr_model import *\n",
    "from src.tokenizer import *\n",
    "from src.audio_handler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894de25c-50f9-45a7-abd5-f8214ddb191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceModel(nn.Module):\n",
    "    def __init__(self, asr_model, tokenizer, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.asr = asr_model\n",
    "        self.vocab = tokenizer.get_vocab()\n",
    "        self.device = device\n",
    "        \n",
    "        self.asr.to(device)\n",
    "        self.asr.eval()\n",
    "        \n",
    "        self.decoder = ctc_decoder(\n",
    "            lexicon = None,\n",
    "            tokens = self.vocab,\n",
    "            blank_token = '<blank>',\n",
    "            sil_token = 'ред',\n",
    "            unk_word = None,\n",
    "            nbest = 1,\n",
    "            beam_size = 50\n",
    "        )\n",
    "    \n",
    "    def forward(self, waveform, sr):\n",
    "        if not isinstance(waveform, torch.Tensor):\n",
    "            waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "        \n",
    "        if waveform.ndim == 2:\n",
    "            if waveform.shape[0] == 2:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "            elif waveform.shape[1] == 2:\n",
    "                waveform = waveform.T\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        elif waveform.ndim == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        \n",
    "        wave_np = waveform.squeeze(0).numpy()\n",
    "        trimmed, _ = librosa.effects.trim(wave_np, top_db=TOP_DB)\n",
    "        waveform = torch.tensor(trimmed, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        max_val = waveform.abs().max()\n",
    "        if max_val > 0:\n",
    "            waveform = waveform / max_val\n",
    "        \n",
    "        if sr != 16_000:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, 16_000)\n",
    "        \n",
    "        waveform = waveform.unsqueeze(0)\n",
    "        waveform = waveform.to(self.device)\n",
    "        \n",
    "        DOWNSAMPLING_FACTOR = 320\n",
    "        raw_length = waveform.shape[-1]\n",
    "        input_length = torch.div(\n",
    "            torch.tensor([raw_length]), \n",
    "            DOWNSAMPLING_FACTOR, \n",
    "            rounding_mode='floor'\n",
    "        ).to(self.device)\n",
    "        input_length[input_length == 0] = 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            log_probs = self.asr(waveform, input_length)\n",
    "            log_probs_cpu = log_probs.transpose(0, 1).contiguous().cpu()\n",
    "            results = self.decoder(log_probs_cpu)\n",
    "            \n",
    "            tokens = results[0][0].tokens\n",
    "            text = ''.join([self.vocab[idx] for idx in tokens])\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def transcribe_file(self, audio_path):\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        return self.forward(waveform, sr)\n",
    "    \n",
    "    def transcribe_batch(self, waveforms, sample_rates):\n",
    "        texts = []\n",
    "        for waveform, sr in zip(waveforms, sample_rates):\n",
    "            text = self.forward(waveform, sr)\n",
    "            texts.append(text)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0bc011-f1f8-408d-9e8d-64bd8b68c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(references, hypotheses):\n",
    "    wer = jiwer.wer(references, hypotheses)\n",
    "    cer = jiwer.cer(references, hypotheses)\n",
    "    \n",
    "    print(f\"WER: {wer*100:.2f}%\")\n",
    "    print(f\"CER: {cer*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\n--- Sample Predictions ---\")\n",
    "    for i in range(min(5, len(references))):\n",
    "        print(f\"\\nRef: {references[i]}\")\n",
    "        print(f\"Hyp: {hypotheses[i]}\")\n",
    "    \n",
    "    return wer, cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a5f4d-98ce-4ea4-a193-2771b7841d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "token_path = os.path.join(\"data\", \"tokenizer.json\")\n",
    "tokenizer = Tokenizer.load(token_path)\n",
    "ssl_model = SSLModel()\n",
    "asr_model = ASRModel(ssl_model, tokenizer.vocab_size)\n",
    "update_ver = 5_000\n",
    "\n",
    "checkpoint_dict = torch.load(os.path.join('models', 'asr_model', f'asr_model_prototype_{update_ver}.pth'))\n",
    "asr_state_dict = checkpoint_dict['model_state_dict']\n",
    "asr_model.load_state_dict(asr_state_dict, strict=True)\n",
    "\n",
    "inf_model = InferenceModel(asr_model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063f95a-87d5-4631-8695-78033a288e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe from file\n",
    "# text = inf_model.transcribe_file('test.flac')\n",
    "# print(f\"Transcription: {text}\")\n",
    "\n",
    "# # Transcribe from waveform\n",
    "waveform, sr = sf.read('test.flac', always_2d=True)\n",
    "waveform = torch.tensor(waveform.T, dtype=torch.float32)\n",
    "text = inf_model(waveform, sr)\n",
    "print(f\"Transcription: {text}\")\n",
    "\n",
    "# Batch transcription\n",
    "# audio_files = ['audio1.wav', 'audio2.wav', 'audio3.wav']\n",
    "# waveforms = []\n",
    "# sample_rates = []\n",
    "\n",
    "# for file in audio_files:\n",
    "#     wav, sr = torchaudio.load(file)\n",
    "#     waveforms.append(wav)\n",
    "#     sample_rates.append(sr)\n",
    "\n",
    "# texts = inference_model.transcribe_batch(waveforms, sample_rates)\n",
    "# for file, text in zip(audio_files, texts):\n",
    "#     print(f\"{file}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8506e-8187-4f48-8c8a-c93d68c1c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "wer, cer = calculate_metrics(references, hypotheses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
